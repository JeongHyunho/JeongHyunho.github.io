---
layout: post
title: "[정리] MIER: Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling (ICML Workshop, 2020)"
---

Author: Russell Mendonca*, Xinyang Geng*, Chelsea Finn, Sergey Levine
Paper Link: https://arxiv.org/abs/2006.07178
Talk in NeurIPS2020 Workshop: https://slideslive.com/38931350/

### 요약

### Abstract
* 여러 태스크를 수행하려 할 때, 다양한 스킬들을 학습하는 것은 많은 수의 샘플이 요구됨
* Meta RL 은 선험지식을 이용해 빠르게 적응(adapt)하지만, 시험 태스크가 학습 태스크들에 얼마나 가까운지에 따라 결과가 상이함
* On policy 처럼 많은 샘플 없이 시험 태스크를 해결하는 것, 즉 효율적인 외적(extrapolate)을 목표로 함
* Dynamics 모델 인식(identification)과 경험 재분류(experience relabeling) 과정으로 목표를 달성하는 방법 MIER 를 제안함
* 모델 학습은 적은 수이고 off policy 인 샘플로 가능한 것에서 착안한 것으로, 시험 태스크에서 학습한 모델을 이용 policy 와 value 를 학습하여, Meta RL 없이 외적 수행함

![Figure 1](/images/MIER/fig1.png)
Fig 1. Meta RL 에서의 모델 인식과 경험 재분류 방법
모델 인식에서 얻은 context variable 과 시험 태스크의 샘플를 이용하여 가상의 경험을 만들고 이로부터 policy 를 학습함

### Introduction
* 선험 지식을 이용하여 새 태스크를 빠르게 학습하는 Meta RL 은 대부분 많은 수의 on policy 샘플들이 요구되었음
* Off policy 기반의 value 를 Meta 학습하여 Policy 를 학습시키는 것은 계산량이 많아 어려움 (Appendix D)
* 대신 Dynamics 와 Reward 모델을 Meta 학습하고 이를 이용하여 policy 와 value 를 도출하는 것으로 Meta RL 을 수행할 것임
* 태스크 정보를 담고 있은 context variable 은 모델의 입력이며, gradient descent 를 이용하여 각 태스크에 적응되어 policy 입력이 됨 (Fig. 1)
* Policy 학습은 context variable 이 state 에 추가됨을 제외하면, standard RL 과 다르지 않음
* 시험 태스크의 context variable 이 상이하면 policy 성능에 문제될 수 있는데, gradient descent 방식으로 모델 적응시키고 이로부터 얻은 가상 경험으로 policy 학습하는 경험 재분류 방법 사용함 (Fig. 1)
(시험 태스크 샘플로 context variable 을 meta learning 방식으로 적응시켰다는 말)

### Preliminaries
* Meta RL 은 표준 RL 에 더하여 태스크 분포 $\rho(\mathcal{T})$를 가지며 아래와 같은 목적함수를 최대화함
단, 시험 태스크에 대해서 policy 의 적응을 위해 $D_{adapt}^{(\mathcal{T})}$ 를 수집함

$$
\mathbb{E}_{\mathcal{T}\sim\rho(\mathcal{T}),\mathbf{s}_t,\mathbf{a}_t\sim\phi_\mathcal{T}}
[\textstyle\sum_{t}\gamma^tr(\mathbf{s}_t,\mathbf{a}_t)]
$$

* Dynamics 모델의 Meta 학습은 [MAML](https://arxiv.org/abs/1703.03400)을 따르며, $D_{adapt}^{(\mathcal{T})}$ 을 이용해 적응한 모델의 $D_{eval}^{(\mathcal{T})}$ 에 대한 손실함수를 이용함 (즉, 적은 데이터로 빠르게 적응해 평가 데이터에 대한 성능을 올리도록 유도함)

$$
\min_{f,\mathcal{A}}
[\mathcal{L}(f(X_\mathcal{T};
\mathcal{A}(\theta,\mathcal{D}^{(\mathcal{T})}_{adapt})), Y_{\mathcal{T}})]
$$

* 적응을 나타내는 $\mathcal{A}(\theta,\mathcal{D}^{(\mathcal{T})}_{adapt})$ 을 한one step 버전으로 나타내면 아래와 같음

$$
\mathcal{A}_{\text{MAML}}(\theta,\mathcal{D}^{(\mathcal{T})}_{adapt}))=
\theta-\alpha\nabla_\theta\mathbb{E}_{X,Y\sim\mathcal{D}^{\mathcal{T}}_{adapt}}
[\mathcal{L}(f(X;\theta),Y)]
$$
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjA0NzUyNDQsMjA1NzE0NDg0OCwtNjEyOT
k0MzM0XX0=
-->